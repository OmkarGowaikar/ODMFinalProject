{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import gym \n",
    "from gym import envs\n",
    "import numpy as np\n",
    "import time\n",
    "def save_frames_as_gif(frames, path='./', filename='gym_animation_mp.gif'):\n",
    "\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleSwarmOptimisation(object):\n",
    "    def __init__(self, fitnessFunction, bounds, numParticles, omega, v, phiL, phiG,size=8):\n",
    "        self.bestGlobalFitness = -np.inf              \n",
    "        self.bestGlobalPos = []     \n",
    "        self.fitnessFunction = fitnessFunction\n",
    "        self.bounds = bounds \n",
    "        self.swarm = [] #Create swarm\n",
    "        for i in range(numParticles):\n",
    "            self.swarm.append(Particle(bounds, omega, v, phiL, phiG,particle_size=size))\n",
    "\n",
    "    def optimize(self, maximumEpochs, target):\n",
    "        bestGlobalFitness = -np.inf              \n",
    "        bestGlobalPos = []                \n",
    "        #Optimisation loop\n",
    "        for i in range(1, maximumEpochs):\n",
    "            #Evaluate each particles fitness\n",
    "            for particle in self.swarm:\n",
    "                fitness = particle.evaluate(self.fitnessFunction)\n",
    "                #Determine if current particle is new global best\n",
    "                if fitness > bestGlobalFitness:\n",
    "                    bestGlobalPos = particle.bestPos.copy()\n",
    "                    bestGlobalFitness = particle.bestFitness\n",
    "                    self.bestGlobalPos=bestGlobalPos\n",
    "            #Update velocity and positions\n",
    "            for p in self.swarm:\n",
    "                p.updateVelocity(bestGlobalPos)\n",
    "                p.updatePosition(self.bounds)     \n",
    "            #Resample best to see if environment is solved\n",
    "            bestGlobalFitness = self.fitnessFunction(bestGlobalPos, 100)\n",
    "            print('Iteration: ' + str(i) + ' Global best: ' + str(bestGlobalFitness))\n",
    "            if bestGlobalFitness > target:\n",
    "                self.bestGlobalPos=bestGlobalPos\n",
    "                return i\n",
    "        return i #failed to solve\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, bounds, omega, v, phiL, phiG,particle_size=8):\n",
    "        self.bestFitness = -np.inf     \n",
    "        self.bestPos = []\n",
    "        self.omega, self.phiL, self.phiG  = omega, phiL, phiG\n",
    "\n",
    "        self.position =  np.random.uniform(low=bounds[0], high=bounds[1], size=particle_size)\n",
    "        self.velocity = np.random.uniform(low=-v, high=v, size=particle_size)\n",
    "\n",
    "    # evaluate current fitness\n",
    "    def evaluate(self, fitnessFunction):\n",
    "        fitness = fitnessFunction(self.position, 25)\n",
    "        #update best position\n",
    "        if fitness > self.bestFitness:\n",
    "            self.bestPos = self.position.copy()\n",
    "            self.bestFitness = fitness\n",
    "        else:\n",
    "            #Re-evaluate best \n",
    "            self.bestFitness = fitnessFunction(self.bestPos, 25)\n",
    "        return self.bestFitness\n",
    "                    \n",
    "    # update new particle velocity\n",
    "    def updateVelocity(self, globalBest):\n",
    "        velLocal= self.phiL * np.random.rand((len(self.bestPos))) * (self.bestPos - self.position)\n",
    "        velGlobal = self.phiG * np.random.rand((len(self.bestPos))) * (globalBest - self.position)\n",
    "        self.velocity = self.omega * self.velocity + velLocal + velGlobal\n",
    "\n",
    "    # update the particle position \n",
    "    def updatePosition(self,bounds):  \n",
    "        self.position = self.position + self.velocity            \n",
    "        self.position[self.position < bounds[0]] = bounds[0]\n",
    "        self.position[self.position > bounds[1]] = bounds[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleFitness(object):\n",
    "\n",
    "    def __init__(self, terminationStep):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.terminationStep = terminationStep\n",
    "\n",
    "    def policy(self, state, pos):\n",
    "        z = state.dot(pos)\n",
    "        exp = np.exp(z)\n",
    "        return exp/np.sum(exp)\n",
    "\n",
    "    def evaluate(self, pos, evaluationIterations):\n",
    "        policy = np.reshape(pos, (4,2))\n",
    "        rewardTotal = 0\n",
    "        for i in range(evaluationIterations):\n",
    "            state = self.env.reset()\n",
    "            step = 0\n",
    "            while True:\n",
    "                step += 1\n",
    "                probs = self.policy(state, policy)\n",
    "                action = np.random.choice(2,p=probs)\n",
    "                state, reward, terminal, _ = self.env.step(action)\n",
    "                rewardTotal += reward\n",
    "                if terminal or step > self.terminationStep:\n",
    "                    break\n",
    "        return rewardTotal/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole = CartpoleFitness(200)\n",
    "\n",
    "weightSpaceBounds = 8\n",
    "numberOfParticles = 15\n",
    "momentum = 0.5\n",
    "initialVelocityBounds = 0.25\n",
    "localWeight = 2\n",
    "globalWeight = 2\n",
    "\n",
    "p = [weightSpaceBounds, numberOfParticles, momentum, initialVelocityBounds, localWeight, globalWeight]\n",
    "\n",
    "timeTotal = 0\n",
    "iterationTotal = 0\n",
    "for i in range(1, 11):\n",
    "    start = time.time()\n",
    "    solver = ParticleSwarmOptimisation(cartpole.evaluate, (-p[0],p[0]), p[1], p[2], p[3], p[4], p[5])\n",
    "    k = solver.optimize(25, 195)\n",
    "    iterationTotal += k\n",
    "    end = time.time() - start\n",
    "    print('Trail ' + str(i) + ' solved in ' + str(k) + ' iterations and ' + \"{:.1f}\".format(end) + ' seconds.')\n",
    "    timeTotal += end\n",
    "print('Solved in an average of ' + str(iterationTotal/i) + ' iterations, with ' + \"{:.1f}\".format(timeTotal /i) + ' seconds per trail.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "frames = []\n",
    "rewards=[]\n",
    "for t in range(1000):\n",
    "    #Render to frames buffer\n",
    "    if t==0:\n",
    "        state=env.reset()\n",
    "    else:\n",
    "        state,reward,\n",
    "    frames.append(env.render(mode=\"rgb_array\"))\n",
    "    z= state.dot(solver.bestGlobalPos.reshape(-1,2))\n",
    "    probs = np.exp(z)/np.sum(np.exp(z))\n",
    "    action = np.random.choice(2,p=probs)\n",
    "    state, reward, terminal, _ = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if terminal:\n",
    "        break\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcrobotFitness(object):\n",
    "\n",
    "    def __init__(self, terminationStep):\n",
    "        self.env = gym.make('Acrobot-v1')\n",
    "        self.terminationStep = terminationStep\n",
    "\n",
    "    def policy(self, state, pos):\n",
    "        z = state.dot(pos)\n",
    "        exp = np.exp(z)\n",
    "        return exp/np.sum(exp)\n",
    "\n",
    "    def evaluate(self, pos, evaluationIterations):\n",
    "        policy = np.reshape(pos, (6,3))\n",
    "        rewardTotal = 0\n",
    "        for i in range(evaluationIterations):\n",
    "            state = self.env.reset()\n",
    "            step = 0\n",
    "            while True:\n",
    "                step += 1\n",
    "                probs = self.policy(state, policy)\n",
    "                action = np.random.choice(3,p=probs)\n",
    "                state, reward, terminal, _ = self.env.step(action)\n",
    "                rewardTotal += reward\n",
    "                if terminal or step > self.terminationStep:\n",
    "                    break\n",
    "        return rewardTotal/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountainCar = AcrobotFitness(200)\n",
    "\n",
    "weightSpaceBounds = 8\n",
    "numberOfParticles = 10\n",
    "momentum = 0.5\n",
    "initialVelocityBounds = 0.25\n",
    "localWeight = 2\n",
    "globalWeight = 2\n",
    "\n",
    "p = [weightSpaceBounds, numberOfParticles, momentum, initialVelocityBounds, localWeight, globalWeight]\n",
    "\n",
    "timeTotal = 0\n",
    "iterationTotal = 0\n",
    "for i in range(1, 11):\n",
    "    start = time.time()\n",
    "    solver = ParticleSwarmOptimisation(mountainCar.evaluate, (-p[0],p[0]), p[1], p[2], p[3], p[4], p[5],size=18)\n",
    "    k = solver.optimize(20, -70)\n",
    "    iterationTotal += k\n",
    "    end = time.time() - start\n",
    "    print('Trail ' + str(i) + ' solved in ' + str(k) + ' iterations and ' + \"{:.1f}\".format(end) + ' seconds.')\n",
    "    timeTotal += end\n",
    "print('Solved in an average of ' + str(iterationTotal/i) + ' iterations, with ' + \"{:.1f}\".format(timeTotal /i) + ' seconds per trail.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "observation = env.reset()\n",
    "frames = []\n",
    "rewards=[]\n",
    "for t in range(1000):\n",
    "    #Render to frames buffer\n",
    "    if t==0:\n",
    "        state=env.reset()\n",
    "    else:\n",
    "        state,reward,\n",
    "    frames.append(env.render(mode=\"rgb_array\"))\n",
    "    z= state.dot(solver.bestGlobalPos.reshape(-1,3))\n",
    "    probs = np.exp(z)/np.sum(np.exp(z))\n",
    "    action = np.random.choice(3,p=probs)\n",
    "    state, reward, terminal, _ = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if terminal:\n",
    "        break\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
